{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa73d44-879d-46c5-ba9c-d53072a63d72",
   "metadata": {},
   "source": [
    "# Review Generator\n",
    "The code takes a dataset with books as input (could also be used for movies or similar). The dataset should include titles, plots, genres and reviews (specified as a number). The code then uses BERT to predict what review a book would get based on the plot, title and genre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a7185-ce1c-4069-910e-2dd8f7aa47a8",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Necessary libraries and modules are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c46569e-5a3b-48af-adef-38a00bebb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "# Limiting libraries to use all but one CPU core, in order to prevent thread oversubscription and improve system responsiveness\n",
    "torch.set_num_threads(max(1, os.cpu_count() - 1))\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(max(1, os.cpu_count() - 1))\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(max(1, os.cpu_count() - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca3dfb-5754-43cd-8347-6ee094114502",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The dataset is loaded and names are changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3f2062-1a39-4369-a370-7055e95ef3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"booksdata.csv\",low_memory=False)\n",
    "title = 'title'\n",
    "plot = 'description'\n",
    "genre = 'categories'\n",
    "review = 'average_rating'\n",
    "\n",
    "titles = books[title].fillna(\"\").astype(str).tolist()\n",
    "plots  = books[plot].fillna(\"\").astype(str).tolist()\n",
    "genres = books[genre].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# Creating pytorch device object, making tensors & models run on CPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a7c65-ca72-41c9-8f22-61f0e30c9b3c",
   "metadata": {},
   "source": [
    "## Create tensors with BERT\n",
    "\n",
    "AutoTokenizer and AutoModel are used to convert strings into tokens and then convert those into contextual embeddings. The pretrained model (\"distilbert-base-uncased\") is a lightweight, fast version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5deea2f5-20aa-46f0-8620-7a2492252eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device).eval()\n",
    "\n",
    "\n",
    "# Function for encoding text into embeddings\n",
    "def encode_text(titles, plots, genres, batch_size=32, max_len=128):\n",
    "    texts = [f\"Title: {t}. Genre: {g}. Plot: {p}\" for t, p, g in zip(titles, plots, genres)]\n",
    "    all_embs = []\n",
    "\n",
    "    with torch.no_grad():     # Disable gradient tracking to save memory and time\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "\n",
    "            # Convert text into token IDs and attention masks that BERT understands\n",
    "            enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "            \n",
    "            # Run BERT model on tokenized inputs\n",
    "            outputs = bert(**enc)\n",
    "\n",
    "            # Extract first token embedding and store it\n",
    "            cls = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embs.append(cls)\n",
    "    \n",
    "    all_embs = np.vstack(all_embs)\n",
    "    return all_embs\n",
    "\n",
    "\n",
    "# Create tensors (x=inputs, y=targets)\n",
    "x = encode_text(titles, plots, genres, batch_size=32, max_len=128)\n",
    "x = torch.from_numpy(x).float()\n",
    "y = books[review].fillna(0).astype(float).to_numpy()\n",
    "y = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedd5f2-2fcc-483e-98e5-aa0b918426c9",
   "metadata": {},
   "source": [
    "## Create datasets\n",
    "\n",
    "A TensorDataset is created and split into train, val and test sets. The number in val and test can be changed to change the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25098181-e03d-4aee-a698-fd7e5a2199e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = TensorDataset(x, y)\n",
    "\n",
    "# Split data\n",
    "val_amount = int(len(full_dataset)*0.15)\n",
    "test_amount = int(len(full_dataset)*0.15)\n",
    "train_amount = len(full_dataset) - val_amount - test_amount\n",
    "train_data, val_data, test_data = random_split(full_dataset, [train_amount, val_amount, test_amount])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d414b9-9b20-46c7-bcbd-a77af1cb268b",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "A simple feedforward neural network is created, using two hidden layers with ReLU activation and dropout for regularization to help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f3aaa4-c0d3-47dd-b637-01866f65a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTReviewPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b768652-076a-44af-a476-1d633804b864",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "A function for training and evaluation the model over multiple epochs is created. The function takes a model, optimizer, loss function, number of epochs, device and dataloaders for train and val sets as input. The function prints the average training and validation losses after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eccb91c2-3fd9-473e-8f16-fd26cce45528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, optimizer, loss_fn, num_epochs, train_dataloader, val_dataloader, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for batch_x, batch_y in train_dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_x)\n",
    "            loss = loss_fn(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_dataloader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "            \n",
    "                pred = model(batch_x)\n",
    "                loss = loss_fn(pred, batch_y)\n",
    "                eval_loss.append(loss.item())\n",
    "\n",
    "        # Printing\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {np.mean(train_loss):.4f} | Val Loss: {np.mean(eval_loss):.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84d9bb-4124-428a-8ca9-e32f10c05c73",
   "metadata": {},
   "source": [
    "## Run training\n",
    "\n",
    "Hyperparameters and model architecture are chosen and the model is created and trained. The Adam optimizer and Mean Squared Error loss was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e19a3a-666e-4caf-98c7-02cd0ce00193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train Loss: 9.4014 | Val Loss: 2.2582\n",
      "Epoch 001 | Train Loss: 0.6006 | Val Loss: 0.3430\n",
      "Epoch 002 | Train Loss: 0.2969 | Val Loss: 0.3281\n",
      "Epoch 003 | Train Loss: 0.2792 | Val Loss: 0.3136\n",
      "Epoch 004 | Train Loss: 0.2814 | Val Loss: 0.3051\n",
      "Epoch 005 | Train Loss: 0.2584 | Val Loss: 0.3010\n",
      "Epoch 006 | Train Loss: 0.2517 | Val Loss: 0.2980\n",
      "Epoch 007 | Train Loss: 0.2470 | Val Loss: 0.2959\n",
      "Epoch 008 | Train Loss: 0.2503 | Val Loss: 0.2946\n",
      "Epoch 009 | Train Loss: 0.2491 | Val Loss: 0.2942\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch = 128\n",
    "num_workers = max(1, min(8, os.cpu_count()-1))\n",
    "loader_train = DataLoader(train_data, batch_size=batch, shuffle=True, num_workers=num_workers)\n",
    "loader_val = DataLoader(val_data, batch_size=batch, num_workers=num_workers)\n",
    "loader_test = DataLoader(test_data, batch_size=batch, num_workers=num_workers)\n",
    "\n",
    "# Create model\n",
    "input_size = 768\n",
    "hidden_size = 256\n",
    "model = BERTReviewPredictor(input_size, hidden_size)\n",
    "\n",
    "# Train model\n",
    "num_epochs = 10\n",
    "lr = 0.0001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "train_eval(model,optimizer,loss_fn,num_epochs,loader_train,loader_val,\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703135b-79fd-446e-8703-93023b7add9c",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The model is evaluated using the test dataset. Mean Squared Error (MSE) and Mean Absolute Error (MAE) are computed to measure how accurate the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5611afab-b88c-4342-8e93-cabe11513fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.2131, MAE: 0.2831\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():    # gradient computation is disabled for efficiency\n",
    "    preds, labels = [], []\n",
    "    for x_batch, y_batch in loader_test:\n",
    "        x_batch = x_batch.to(device)\n",
    "        pred = model(x_batch).cpu()\n",
    "        preds.append(pred)\n",
    "        labels.append(y_batch)\n",
    "    preds = torch.cat(preds).squeeze()\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "# Calculate and print MSE & MAE\n",
    "mse = nn.MSELoss()(preds, labels)\n",
    "mae = torch.mean(torch.abs(preds - labels))\n",
    "print(f\"Test MSE: {mse:.4f}, MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef126a55-cc86-46e3-9991-e95eb70ee3ca",
   "metadata": {},
   "source": [
    "## Test predictions\n",
    "\n",
    "A function is created to allow for predicting the review of a specific book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a412dd78-6172-46ca-b1a6-eac511277785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.022887706756592\n"
     ]
    }
   ],
   "source": [
    "def predict_book(title: str, plot: str, genre: str, model):\n",
    "    model.eval()\n",
    "    xp = encode_text([title], [plot], [genre])\n",
    "    xp = torch.from_numpy(xp).float()\n",
    "    device = next(model.parameters()).device\n",
    "    xp = xp.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(xp)\n",
    "    if pred.numel() == 1:\n",
    "        return pred.item()\n",
    "    else:\n",
    "        return pred.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "print(predict_book(\n",
    "    title=\"Harry Potter\",\n",
    "    plot=\"A young wizard.\",\n",
    "    genre=\"fantasy\",\n",
    "    model=model\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d16e9a-72d5-407f-9a29-d92df31e997a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
